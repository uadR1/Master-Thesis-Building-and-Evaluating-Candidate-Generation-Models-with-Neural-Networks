{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas & Numpy\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages \n",
    "\n",
    "# Evaluation \n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    precision_at_k,\n",
    "    recall_at_k,\n",
    "    ndcg_at_k,\n",
    "    map_at_k, \n",
    "    get_top_k_items,\n",
    "    rmse,\n",
    "    mae,\n",
    "    rsquared,\n",
    "    exp_var\n",
    ")\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# Recomender Utilities\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.ncf.ncf_singlenode import NCF\n",
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "\n",
    "# System & OS\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "#import papermill as pm\n",
    "import pandas as pd\n",
    "\n",
    "# Turn of Warnings for Readability \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Train & Test Data\n",
    "train = pd.read_csv(\"../../00_Data/online_retail_train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"../../00_Data/online_retail_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train: (206861, 3)\n",
      "Shape of Test: \t (51879, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check Shapes\n",
    "print(\"Shape of Train:\", train.shape)\n",
    "print(\"Shape of Test: \\t\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users in Train: 3690\n",
      "Unique Users in Test: 3690\n",
      "Unique Items in Train: 2746\n",
      "Unique Items in Test: 2746\n"
     ]
    }
   ],
   "source": [
    "# Check Number of Unique Items and User in Train & Test \n",
    "print(\"Unique Users in Train:\", train.CustomerID.nunique())\n",
    "print(\"Unique Users in Test:\", test.CustomerID.nunique())\n",
    "print(\"Unique Items in Train:\", train.StockCode.nunique())\n",
    "print(\"Unique Items in Test:\", test.StockCode.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of columns \n",
    "train = train.rename(columns={'StockCode':'itemID', 'CustomerID':'userID', 'purchased':'rating'})\n",
    "test = test.rename(columns={'StockCode':'itemID', 'CustomerID':'userID', 'purchased':'rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Dataset to CSV files. This is a pre-step for the NCF Dataset preparation\n",
    "train_file = \"./or_train.csv\"\n",
    "test_file = \"./or_test.csv\"\n",
    "train.to_csv(train_file, index=False)\n",
    "test.to_csv(test_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.dataset:Indexing ./or_train.csv ...\n",
      "INFO:recommenders.models.ncf.dataset:Indexing ./or_test.csv ...\n",
      "INFO:recommenders.models.ncf.dataset:Creating full leave-one-out test file ./or_test_full.csv ...\n",
      "100%|██████████| 3690/3690 [00:40<00:00, 90.94it/s] \n",
      "INFO:recommenders.models.ncf.dataset:Indexing ./or_test_full.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Create the NCF Dataset \n",
    "data = NCFDataset(train_file=train_file, test_file=test_file, seed=1, overwrite_test_file_full=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 13:45:25.024116: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-19 13:45:25.035937: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n"
     ]
    }
   ],
   "source": [
    "# Initiate Model with pretrained NeuMF\n",
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=50,\n",
    "    batch_size=50,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    "    seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 10 [20.38s]: train_loss = 0.370194 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 20 [21.03s]: train_loss = 0.364789 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 30 [20.67s]: train_loss = 0.361750 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 40 [21.02s]: train_loss = 0.359382 \n",
      "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 50 [21.20s]: train_loss = 0.358611 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1061.36715395801 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "# Fit the Modek \n",
    "with Timer() as train_time:\n",
    "    model.fit(data)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1249</td>\n",
       "      <td>0.367157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>396</td>\n",
       "      <td>0.077538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.327272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>912</td>\n",
       "      <td>0.461480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1032</td>\n",
       "      <td>0.273281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  prediction\n",
       "0       1    1249    0.367157\n",
       "1       1     396    0.077538\n",
       "2       1    2601    0.327272\n",
       "3       1     912    0.461480\n",
       "4       1    1032    0.273281"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict all user and items pairings in Test \n",
    "predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n",
    "               for (_, row) in test.iterrows()]\n",
    "\n",
    "# Create a Datafragme from the Predictions \n",
    "predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['prediction'] = predictions['prediction'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31667919582104515"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test.rating, predictions.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 47.797705667006085 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Predict ALL User & Item Pairings \n",
    "with Timer() as test_time:\n",
    "\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(train.itemID.unique())\n",
    "    for user in train.userID.unique():\n",
    "        user = [user] * len(item) \n",
    "        users.extend(user)\n",
    "        items.extend(item)\n",
    "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K:\t0.084282\n",
      "Recall@K:\t0.073742\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Recall & Preicison at 10 \n",
    "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=10)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=10)\n",
    "\n",
    "print(\"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4]\n",
      "[4, 6, 8]\n",
      "[7, 10, 15]\n"
     ]
    }
   ],
   "source": [
    "# Initiate Lists for Hyperparameters \n",
    "factors = [4, 16, 50, 100]\n",
    "layers = [[16, 8, 4], [32, 16, 8], [200, 100, 50], [400, 200, 100]]\n",
    "lr_rates = [0.001, 0.005, 0.007]\n",
    "\n",
    "# Initiate Lists for Assessment\n",
    "accuracy = []\n",
    "recall_10 = []\n",
    "recall_20 = []\n",
    "precision_10 = []\n",
    "precision_20 = []\n",
    "num_factors = []\n",
    "layer_shapes = []\n",
    "learning_rates = []\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17afcd9f6960de0a3656d2c4c5dd434deed0eab3cd38c55c3169df3bef50250d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
