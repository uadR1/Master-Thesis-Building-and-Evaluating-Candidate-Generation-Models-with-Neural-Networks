{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#RBM \n",
    "from recommenders.models.rbm.rbm import RBM\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.plot import line_graph\n",
    "\n",
    "# Evaluation\n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    precision_at_k,\n",
    "    recall_at_k,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Train & Test Data\n",
    "train = pd.read_csv(\"../../00_Data/online_retail_train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"../../00_Data/online_retail_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train: (207084, 3)\n",
      "Shape of Test: \t (51892, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check Shapes\n",
    "print(\"Shape of Train:\", train.shape)\n",
    "print(\"Shape of Test: \\t\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users in Train: 3692\n",
      "Unique Users in Test: 3692\n",
      "Unique Items in Train: 2716\n",
      "Unique Items in Test: 2716\n"
     ]
    }
   ],
   "source": [
    "# Check Number of Unique Items and User in Train & Test \n",
    "print(\"Unique Users in Train:\", train.CustomerID.nunique())\n",
    "print(\"Unique Users in Test:\", test.CustomerID.nunique())\n",
    "print(\"Unique Items in Train:\", train.StockCode.nunique())\n",
    "print(\"Unique Items in Test:\", test.StockCode.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Datasets for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Affinity Matrix\n",
    "from recommenders.datasets.sparse import AffinityMatrix\n",
    "\n",
    "\n",
    "#to use standard names across the analysis \n",
    "header = {\n",
    "        \"col_user\": \"CustomerID\",\n",
    "        \"col_item\": \"StockCode\",\n",
    "        \"col_rating\": \"purchased\",\n",
    "    }\n",
    "\n",
    "#instantiate the sparse matrix generation  \n",
    "am_train = AffinityMatrix(df = train, col_user='CustomerID', col_item='StockCode', col_rating='purchased')\n",
    "am_test = AffinityMatrix(df = test, col_user='CustomerID', col_item='StockCode', col_rating='purchased')\n",
    "\n",
    "#obtain the sparse matrix \n",
    "Xtr, _, _ = am_train.gen_affinity_matrix()\n",
    "Xtst, _, _ = am_test.gen_affinity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3692, 2716)\n",
      "(3692, 2716)\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Print Shape of Train & Test Matrix \n",
    "print(Xtr.shape)\n",
    "print(Xtst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207084\n",
      "51892\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(Xtr))\n",
    "print(np.sum(Xtst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction: Test Only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for Test Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del(model)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#First we initialize the model class\n",
    "model = RBM(\n",
    "    possible_ratings=[1., 5.],\n",
    "    visible_units=Xtr.shape[1],\n",
    "    hidden_units=600,\n",
    "    training_epoch=30,\n",
    "    minibatch_size=100,\n",
    "    keep_prob=0.7,\n",
    "    with_metrics=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 40.06 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "#Model Fit\n",
    "with Timer() as train_time:\n",
    "    model.fit(Xtr)\n",
    "\n",
    "print(\"Took {:.2f} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions for entire Test Matrix\n",
    "pred_test = model.predict(Xtst) ## Always Use Xtst for prediction - better results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Back Pred Test\n",
    "pred_test = am_test.map_back_sparse(pred_test, kind = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    9050628\n",
       "1.0     976844\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value counts in Pred Test: Entire Dataset\n",
    "pred_test.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test_pred with test\n",
    "test_prediction_am = test.merge(pred_test, on=['StockCode', 'CustomerID'], how='left')\n",
    "\n",
    "# Change Purchased to 5 \n",
    "test_prediction_am['purchased'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StockCode</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>purchased</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1249</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2601</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StockCode  CustomerID  purchased  prediction\n",
       "0       1249           1          5         5.0\n",
       "1        396           1          5         5.0\n",
       "2       2601           1          5         5.0\n",
       "3        912           1          5         5.0\n",
       "4       1032           1          5         5.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first five rows \n",
    "test_prediction_am.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    51830\n",
       "1.0       62\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Distribution of Prediction Values in Test Set\n",
    "test_prediction_am.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988052108224774"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy \n",
    "accuracy_score(test_prediction_am.purchased, test_prediction_am.prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction: Top N"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for TOP N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5]\n",
      "[1 5]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Xtr & Xtst before reloading & Transformation\n",
    "print(np.unique(Xtr))\n",
    "print(np.unique(Xtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the sparse matrix Again  \n",
    "Xtr, _, _ = am_train.gen_affinity_matrix()\n",
    "Xtst, _, _ = am_test.gen_affinity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Values above 0 to 5 for Top N training\n",
    "Xtst = np.where(Xtst > 0, 5, Xtst)\n",
    "Xtr = np.where(Xtr > 0, 5, Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5]\n",
      "[0 5]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Xtr & Xtst After reloading & Transformation\n",
    "print(np.unique(Xtr))\n",
    "print(np.unique(Xtst))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for TOP N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 41.29 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "# Delete First Model \n",
    "try:\n",
    "    del(model)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# First we initialize the model class\n",
    "model = RBM(\n",
    "    possible_ratings=[1., 5.], # Always provide this range - way better results! \n",
    "    visible_units=Xtr.shape[1],\n",
    "    hidden_units=600,\n",
    "    training_epoch=30,\n",
    "    minibatch_size=100,\n",
    "    keep_prob=0.7,\n",
    "    with_metrics=True,\n",
    "    seed=42)\n",
    "\n",
    "#Model Fit\n",
    "with Timer() as train_time:\n",
    "    model.fit(Xtr)\n",
    "\n",
    "print(\"Took {:.2f} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.80 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Model prediction on the test set Xtst.\n",
    "with Timer() as prediction_time:\n",
    "    top_k_20 =  model.recommend_k_items(Xtst, top_k=20, remove_seen=True) # Xtst best\n",
    "\n",
    "print(\"Took {:.2f} seconds for prediction.\".format(prediction_time.interval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Back Xtst & Top_k \n",
    "df_top_k_20 = am_test.map_back_sparse(top_k_20, kind = 'prediction')\n",
    "df_test = am_test.map_back_sparse(Xtst, kind = 'ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41936458053028974"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_recall = recall_at_k(df_test, df_top_k_20, col_user=\"CustomerID\", col_item=\"StockCode\", \n",
    "                                    col_rating=\"purchased\", col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\", k=20)\n",
    "eval_recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17afcd9f6960de0a3656d2c4c5dd434deed0eab3cd38c55c3169df3bef50250d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
